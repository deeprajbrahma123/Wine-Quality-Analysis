{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deeprajbrahma123/Wine-Quality-Analysis/blob/main/Wine_Quality_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('red_wine.csv')\n",
        "\n",
        "# Fill missing values with the column mean\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Outlier detection\n",
        "outlier_detector = IsolationForest(contamination=0.01)  # Adjust the contamination parameter as needed\n",
        "outlier_mask = outlier_detector.fit_predict(X)\n",
        "X = X[outlier_mask == 1]\n",
        "y = y[outlier_mask == 1]\n",
        "\n",
        "# Feature selection\n",
        "selector = SelectKBest(score_func=f_regression, k=7)  # Adjust the k value as needed\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the input variables\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdJab-6tleE-",
        "outputId": "b1809246-22aa-4dd6-a6af-dead7c137e18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.46936739343337924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('red_wine.csv')\n",
        "\n",
        "# Handle missing values\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# Outlier detection\n",
        "outlier_detector = IsolationForest(contamination=0.01)  # Adjust the contamination parameter as needed\n",
        "outlier_mask = outlier_detector.fit_predict(X)\n",
        "X = X[outlier_mask == 1]\n",
        "y = y[outlier_mask == 1]\n",
        "\n",
        "# Feature selection\n",
        "selector = SelectKBest(score_func=f_regression, k=7)  # Adjust the k value as needed\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the input variables\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Hyperparameter tuning for Random Forest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 4, 8],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "rf_grid_search = GridSearchCV(RandomForestRegressor(),\n",
        "                              rf_param_grid, scoring='neg_mean_squared_error',\n",
        "                              cv=5)\n",
        "rf_grid_search.fit(X_train_scaled, y_train)\n",
        "best_rf_model = rf_grid_search.best_estimator_\n",
        "print(\"Best Random Forest Model:\", best_rf_model)\n",
        "\n",
        "# Hyperparameter tuning for Gradient Boosting\n",
        "gb_param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, None],\n",
        "    'learning_rate': [0.1, 0.01, 0.001],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "gb_grid_search = GridSearchCV(GradientBoostingRegressor(), gb_param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "gb_grid_search.fit(X_train_scaled, y_train)\n",
        "best_gb_model = gb_grid_search.best_estimator_\n",
        "print(\"Best Gradient Boosting Model:\", best_gb_model)\n",
        "\n",
        "# Train the best models\n",
        "best_rf_model.fit(X_train_scaled, y_train)\n",
        "best_gb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the best models\n",
        "rf_y_pred = best_rf_model.predict(X_test_scaled)\n",
        "rf_mse = mean_squared_error(y_test, rf_y_pred)\n",
        "print(\"Random Forest Mean Squared Error:\", rf_mse)\n",
        "\n",
        "gb_y_pred = best_gb_model.predict(X_test_scaled)\n",
        "gb_mse = mean_squared_error(y_test, gb_y_pred)\n",
        "print(\"Gradient Boosting Mean Squared Error:\", gb_mse)\n",
        "\n",
        "# Ensemble modeling using VotingRegressor\n",
        "ensemble_model = VotingRegressor(estimators=[('Random Forest', best_rf_model), ('Gradient Boosting', best_gb_model)])\n",
        "ensemble_model.fit(X_train_scaled, y_train)\n",
        "ensemble_y_pred = ensemble_model.predict(X_test_scaled)\n",
        "ensemble_mse = mean_squared_error(y_test, ensemble_y_pred)\n",
        "print(\"Ensemble Model Mean Squared Error:\", ensemble_mse)\n",
        "\n",
        "# Investigate feature importance\n",
        "importance = permutation_importance(ensemble_model, X_train_scaled, y_train)\n",
        "feature_importance = importance.importances_mean\n",
        "feature_names = X.columns[selector.get_support()]\n",
        "sorted_indices = np.argsort(feature_importance)[::-1]\n",
        "\n",
        "print(\"Feature Importance:\")\n",
        "for idx in sorted_indices:\n",
        "    print(f\"{feature_names[idx]}: {feature_importance[idx]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4DI4RCWm3R5",
        "outputId": "d81d2294-16b0-4eeb-dac9-cd0c4c559e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Random Forest Model: RandomForestRegressor(n_estimators=300)\n",
            "Best Gradient Boosting Model: GradientBoostingRegressor(learning_rate=0.01, max_depth=5, n_estimators=300,\n",
            "                          subsample=0.8)\n",
            "Random Forest Mean Squared Error: 0.43284807644489826\n",
            "Gradient Boosting Mean Squared Error: 0.45196271769707336\n",
            "Ensemble Model Mean Squared Error: 0.4369984672462357\n",
            "Feature Importance:\n",
            "alcohol: 0.5460536173832518\n",
            "volatile acidity: 0.3338828509143254\n",
            "sulphates: 0.32037786779422434\n",
            "total sulfur dioxide: 0.19422951293992735\n",
            "density: 0.11287316562774778\n",
            "citric acid: 0.10595319431804695\n",
            "pH: 0.013840394309136018\n"
          ]
        }
      ]
    }
  ]
}